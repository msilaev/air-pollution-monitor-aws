import datetime as dt
import io
import logging
import os
import sys
from datetime import datetime
from zoneinfo import ZoneInfo

import boto3
import pandas as pd
from geopy.geocoders import Nominatim

from scripts.data_from_stations import get_air_pollution_data_timeInterval
from src.config import (  # DO NOT MODIFY: Required for imports
    INTERIM_DATA_DIR,
    PROJ_ROOT,
    RAW_DATA_DIR,
)

sys.path.append(PROJ_ROOT)  # DO NOT MODIFY: Required for imports


class DataIngestion:
    def __init__(self, use_s3=False, address="Helsinki"):
        self.logger = logging.getLogger(__name__)
        self.address = address
        self.use_s3 = use_s3

        print(
            f"DataIngestion initialized with use_s3={self.use_s3}, address={self.address}"
        )
        if use_s3:
            self.s3_client = boto3.client("s3")
            self.bucket = os.environ.get("AWS_S3_DATA_BUCKET", "air-pollution-data")
        self.logger = logging.getLogger(__name__)

        self.square_side = 20  # km

        self.air_pollution_stations = [
            "Helsinki Kallio 2",
            "Espoo Leppävaara Läkkisepänkuja",
            "Espoo Luukki",
            "Helsinki Mannerheimintie",
            "Vantaa Tikkurila Neilikkatie",
            "Vantaa Kehä III Viinikkala",
            "Helsinki Kustaa Vaasan tie",
        ]

        self.air_pollution_indicators = [
            "Nitrogen dioxide",
            "Particulate matter < 10 µm",
            "Particulate matter < 2.5 µm",
        ]

    def fetch_pollution_data(
        self, data_type="training", chunk_size_hours=24 * 7, week_number=8
    ):  # noqa: C901
        """Fetch latest pollution data from APIs (from your notebook logic)"""
        try:
            geolocator = Nominatim(user_agent="ny_explorer")
            location = geolocator.geocode(self.address)
            latitude_city = location.latitude
            longitude_city = location.longitude

            air_pollution_total = {}

            for n in range(week_number):
                # Get current UTC time and shift to Helsinki time
                now_utc = dt.datetime.now(dt.timezone.utc)
                now_hel = now_utc + dt.timedelta(hours=3)

                start = now_hel - (n + 1) * dt.timedelta(hours=chunk_size_hours)
                end = now_hel - n * dt.timedelta(hours=chunk_size_hours)

                start = start.replace(tzinfo=None)
                # start = start.strftime('%Y-%m-%d %H:%M:%S.%f')

                end = end.replace(tzinfo=None)
                # end = end.strftime('%Y-%m-%d %H:%M:%S.%f')

                print(f"Fetching data for {start.isoformat()} to {end.isoformat()}")

                air_pollution_week = get_air_pollution_data_timeInterval(
                    latitude_city,
                    longitude_city,
                    square_side=self.square_side,
                    start=start,
                    end=end,
                )

                air_pollution_week = list(air_pollution_week)[0]

                for key in air_pollution_week.keys():
                    if key not in air_pollution_total.keys():
                        air_pollution_total[key] = air_pollution_week[key]
                    else:
                        air_pollution_total[key] = (
                            air_pollution_total[key] + air_pollution_week[key]
                        )

            df_air_pollution_total = pd.DataFrame()
            # Loop through all air pollution stations
            for station in self.air_pollution_stations:
                merged_df_list = []
                timestamp = []

                # Process data for the current station
                for x in air_pollution_total[station]:
                    df = pd.DataFrame(x.values())

                    # Convert nested values to floats
                    for col in df.columns:
                        if col != "Timestamp":
                            df[col] = df[col].apply(lambda x: x["value"])
                            df[col] = df[col].astype(float)

                    merged_df_list.append(df)
                    timestamp.append(list(x.keys())[0])

                # Combine all data for the current station
                merged_df = pd.concat(merged_df_list, ignore_index=True)
                merged_df["Timestamp"] = timestamp
                merged_df["Timestamp"] = pd.to_datetime(merged_df["Timestamp"])
                # merged_df["Station"] = station

                # Select relevant columns
                merged_df = merged_df[["Timestamp"] + self.air_pollution_indicators]
                merged_df = merged_df.drop_duplicates(subset=["Timestamp"])
                merged_df.sort_values(by="Timestamp", inplace=True)

                ###############
                df_air_pollution = merged_df.copy()

                df_air_pollution = df_air_pollution.rename(
                    columns={
                        "Particulate matter < 10 µm": f"Particulate matter < 10 µm_{station}",
                        "Particulate matter < 2.5 µm": f"Particulate matter < 2.5 µm_{station}",
                        "Nitrogen dioxide": f"Nitrogen dioxide_{station}",
                    }
                )
                if df_air_pollution_total.empty:
                    df_air_pollution_total = df_air_pollution.copy()
                else:
                    df_air_pollution_total = df_air_pollution_total.merge(
                        df_air_pollution, how="outer", on="Timestamp"
                    )  # , suffixes=('', f'_{station}'))

                # Save to parquet file with station name in the filename
                if self.use_s3:
                    filename = f"training_data/{station.replace(' ', '_')}_air_pollution_data_{data_type}.parquet"
                    self.upload_to_s3(merged_df, filename)

                    print(
                        f"Saved data for station: {station} to {filename}, length: {len(merged_df)} in s3"
                    )
                else:
                    filename = f"{station.replace(' ', '_')}_air_pollution_data_{data_type}.parquet"
                    full_path = os.path.join(RAW_DATA_DIR, filename)

                    merged_df.to_parquet(full_path, index=False)
                    print(
                        f"Saved data for station: {station} to {filename}, length: {len(merged_df)} in {full_path}"
                    )

        except Exception as e:
            self.logger.error(f"Failed to fetch data: {e}")
            raise

        # Save to parquet file with station name in the filename
        if self.use_s3:
            filename = f"{data_type}_data/air_pollution_data_{data_type}_total.parquet"
            self.upload_to_s3(df_air_pollution_total, filename)
            print(
                f"Saved total data: to {filename}, length: {len(merged_df)}, "
                f"date range: {df_air_pollution_total['Timestamp'].min()} to {df_air_pollution_total['Timestamp'].max()} in s3"
            )

        else:
            filename = f"air_pollution_data_{data_type}_total.parquet"
            full_path = os.path.join(INTERIM_DATA_DIR, filename)

            df_air_pollution_total.to_parquet(full_path, index=False)
            print(
                f"Saved total data: to {filename}, length: {len(merged_df)}, "
                f"date range: {df_air_pollution_total['Timestamp'].min()} to {df_air_pollution_total['Timestamp'].max()} in {full_path}"
            )

    def upload_to_s3(self, df, key):
        """Upload DataFrame to S3 as parquet file"""
        try:
            s3_client = boto3.client("s3")

            bucket = os.environ.get("AWS_S3_DATA_BUCKET", "air-pollution-models")
            bucket = bucket.replace("s3://", "").strip()

            print(f"Uploading data to s3://{bucket}/{key}")

            # print(credentials.access_key, credentials.secret_key, credentials.token)
            # print(f"Uploading data to s3://{bucket}/{key}")
            # Convert DataFrame to parquet in memory
            buffer = io.BytesIO()
            df.to_parquet(buffer, index=False)
            buffer.seek(0)

            s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=buffer.getvalue(),
                ContentType="application/octet-stream",
            )

            self.logger.info(f"Uploaded data to s3://{bucket}/{key}")
        except Exception as e:
            self.logger.error(f"Failed to upload data to S3: {e}")
            raise

    def merge_and_save_data_local(self):
        # Loop through all air pollution stations
        df_air_pollution_total = pd.DataFrame()

        for station in self.air_pollution_stations:
            # Generate the filename for the station
            filename = f"{station.replace(' ', '_')}_air_pollution_data.parquet"
            full_path = os.path.join(RAW_DATA_DIR, filename)

            # Load the air pollution dataset for the station
            if os.path.exists(full_path):
                df_air_pollution = pd.read_parquet(full_path)
                df_air_pollution["Timestamp"] = pd.to_datetime(
                    df_air_pollution["Timestamp"]
                )

                # df_air_pollution.ffill(inplace=True)
                # df_air_pollution.bfill(inplace=True)

                print(f"Loaded data for station: {station}")

                df_air_pollution = df_air_pollution[
                    [
                        "Timestamp",
                        "Particulate matter < 10 µm",
                        "Particulate matter < 2.5 µm",
                        "Nitrogen dioxide",
                    ]
                ]

                df_air_pollution = df_air_pollution.rename(
                    columns={
                        "Particulate matter < 10 µm": f"Particulate matter < 10 µm_{station}",
                        "Particulate matter < 2.5 µm": f"Particulate matter < 2.5 µm_{station}",
                        "Nitrogen dioxide": f"Nitrogen dioxide_{station}",
                    }
                )

                if df_air_pollution_total.empty:
                    df_air_pollution_total = df_air_pollution.copy()
                else:
                    df_air_pollution_total = df_air_pollution_total.merge(
                        df_air_pollution, how="outer", on="Timestamp"
                    )  # , suffixes=('', f'_{station}'))

            else:
                print(f"File not found for station: {station}")
                continue

        df_air_pollution_total.to_parquet(
            os.path.join(INTERIM_DATA_DIR, "air_pollution_data_total.parquet"),
            index=False,
        )

    def _clean_and_standardize(self, df):
        """Clean and standardize the collected data"""
        # Remove duplicates
        df = df.drop_duplicates(subset=["Timestamp", "Station"])

        # Sort by timestamp
        df = df.sort_values("Timestamp")

        # Handle missing values
        numeric_columns = df.select_dtypes(include=["float64", "int64"]).columns
        for col in numeric_columns:
            # Forward fill missing values (or use your preferred method)
            df[col] = df[col].fillna(method="ffill")

        # Ensure timestamp is timezone aware
        if df["Timestamp"].dt.tz is None:
            df["Timestamp"] = df["Timestamp"].dt.tz_localize("Europe/Helsinki")

        return df

    def _save_latest_data(self, df):
        """Save latest data to file"""
        os.makedirs(INTERIM_DATA_DIR, exist_ok=True)

        # Save with timestamp
        timestamp = datetime.now(ZoneInfo("Europe/Helsinki")).strftime("%Y%m%d_%H%M%S")
        filename = f"latest_pollution_data_{timestamp}.parquet"
        filepath = os.path.join(INTERIM_DATA_DIR, filename)

        df.to_parquet(filepath, index=False)

        # Also save as 'latest.parquet' for easy access
        latest_filepath = os.path.join(
            INTERIM_DATA_DIR, "latest_pollution_data.parquet"
        )
        df.to_parquet(latest_filepath, index=False)

        self.logger.info(f"Saved latest data to {filepath}")
